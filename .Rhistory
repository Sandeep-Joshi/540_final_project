# Do sentiment analysis for each reply
replies <- cbind(replies, bind_rows(lapply(replies$Text, sentiment_score)))
sprintf("Sentiment analysis done for all replies on Tweet %s\n", tweet_id)
# Do Topic modeling for all replies.
temp_model <- topic_model(replies$Text)
# Add top terms in all replies corpus to Tweet data
tweet_str$Topic_terms_1 <- toString(temp_model$terms[,1])
tweet_str$Topic_terms_2 <- toString(temp_model$terms[,2])
# Add Topic category to the reply, determined by highest beta value. Equal values are not classified.
temp_topics <- temp_model$topics %>% group_by(document) %>%
filter(gamma==max(gamma)) %>%
arrange(document, topic)
temp_topics <- temp_topics %>% filter(gamma != 0.5000000) %>% select(document, topic)
for (j in 1:nrow(replies)) {
replies[j,]$Topic_category <- toString(temp_topics[temp_topics$document==toString(j),]$topic)
}
# remove temp_model from memory to save space
rm(list=c('temp_model', 'temp_topics'))
sprintf("Topic Modelling done for Tweet %s", tweet_id)
return(replies)
}
}
# Tweet: 1335971721262796801
reply_model_1335984976521801733 <- get_all_modeling_data('1335984976521801733')
View(tweets_df)
View(tweets_df)
rm(list = ls(all.names = TRUE))
library(files)
library(googleLanguageR)
library(tidytext)
library(stringr)
library(tidyr)
library(tm)
library(syuzhet)
library(topicmodels)
library(tidyverse)
library(slam)
library(textstem)
library(reshape2)
library(reticulate)
replies_index = list.files('replies')
# load tweet and its replies.
tweets_df <-  read.csv('Practice Tweets.csv')
tweets_df$Topic_terms_2 = tweets_df$Topic_terms_1 = ''  # instantiate new column for storing Topic models
# Clean tweets
clean_replies <- function(reply) {
clean_tweet <- str_remove_all(reply, "@.*?(\\s+?|$)|^RT |http.*?(\\s+?|$)") # To remove @twitter handles and Retweet tag.
clean_tweet <- tolower(clean_tweet)
clean_tweet <- str_squish(str_trim(clean_tweet))
# Not removing punctuations as some libraries use emoticons to analyze tweets
clean_tweet <- removeWords(clean_tweet, stopwords('en')) # delete stopwords from text
clean_tweet <- stem_strings(clean_tweet) # stemming. Might need to remove it as it can confuse certain sentiment analyses
return(clean_tweet)
}
# Check if we have replies data for the tweet using conversation id
check_replies <- function(tweet) {
reply_file = paste0(tweet[,3], ".csv")
if (reply_file %in% replies_index){
# reply found, work with tweet
cat("Tweet: ", tweet[,8], " ")
cat("Likes: ", tweet[,7], "and Retweets: ", tweet[,6], ". ")
reply_file <- paste0('replies/', reply_file)
cat("Found replies file: ", reply_file)
reply_df <- read.csv(reply_file)
cat("Number of replies found: ", nrow(reply_df), ". \n")
reply_df$Text <- lapply(reply_df$Text, clean_replies)
return(reply_df)
}
else {
return(NULL)
}
}
sentiment_score <- function(tweet) {
# Syuzhet. Other methods are “bing”, “afinn”, “nrc”, and “stanford”
# browser()
return(get_nrc_sentiment(tweet))
}
# Topic modeling on tweets to 2 topics
topic_model <- function(tweet_col) {
reply_corpus <- Corpus(VectorSource(tweet_col))
# Stemming, stop words have been taken care of earlier
import_mat <- DocumentTermMatrix(reply_corpus, control = list(stemming =TRUE,
stopwords = TRUE,
minWordLength = 3, # This keeps the data out of sync
removeNumbers = TRUE,
removePunctuation =TRUE))
import_weight = tapply(import_mat$v/row_sums(import_mat)[import_mat$i],
import_mat$j,
mean) *
log2(nDocs(import_mat)/col_sums(import_mat > 0))
#ignore very frequent and 0 terms
import_mat = import_mat[ row_sums(import_mat) > 0, ]
# 2 topics and random seed of 7
LDA_gibbs = LDA(import_mat, k = 2, method = "Gibbs",
control = list(seed = 7, burnin = 1000,
thin = 100, iter = 1000))
# See Topic modelings values to verify all indicators are within control and expected range
cat("LDA Gibb's Alpha: ", LDA_gibbs@alpha,
"entropy: ", mean(apply(posterior(LDA_gibbs)$topics, 1, function(z) - sum(z * log(z)))))
# Return lists from models so that they can be added to df. Complete model doesn't work
return(list("topics" = tidy(LDA_gibbs, matrix = "gamma"), "terms" = terms(LDA_gibbs, 20)))
}
library(reticulate)
py_config()
use_python('C:/Users/Sandeep/anaconda3/envs/r-reticulate/python.exe')
get_all_modeling_data <- function(tweet_id) {
sprintf("For tweet id: %s ", tweet_id)
tweet_str <- tweets_df[tweets_df$ID == tweet_id,]
replies = check_replies(tweet_str)
# Delete blank replies as this causes issues later on with DF operations.
replies <- replies[!grepl("^\\s*$",replies$Text),]
if (!is.null(replies)) {
replies$Topic_category <- 0
# Do sentiment analysis for each reply
replies <- cbind(replies, bind_rows(lapply(replies$Text, sentiment_score)))
sprintf("Sentiment analysis done for all replies on Tweet %s\n", tweet_id)
# Do Topic modeling for all replies.
temp_model <- topic_model(replies$Text)
# Add top terms in all replies corpus to Tweet data
tweet_str$Topic_terms_1 <- toString(temp_model$terms[,1])
tweet_str$Topic_terms_2 <- toString(temp_model$terms[,2])
# Add Topic category to the reply, determined by highest beta value. Equal values are not classified.
temp_topics <- temp_model$topics %>% group_by(document) %>%
filter(gamma==max(gamma)) %>%
arrange(document, topic)
temp_topics <- temp_topics %>% filter(gamma != 0.5000000) %>% select(document, topic)
for (j in 1:nrow(replies)) {
replies[j,]$Topic_category <- toString(temp_topics[temp_topics$document==toString(j),]$topic)
}
# remove temp_model from memory to save space
rm(list=c('temp_model', 'temp_topics'))
sprintf("Topic Modelling done for Tweet %s", tweet_id)
return(replies)
}
}
# Tweet: 1335775535717289984
reply_model_1335775535717289984 <- get_all_modeling_data('1335775535717289984')
View(tweets_df)
View(tweets_df)
View(reply_model_1335775535717289984)
# Tweet: 1335775535717289984
reply_model_1335775535717289984 <- get_all_modeling_data('1335775535717289984')
View(tweets_df)
get_all_modeling_data <- function(tweet_id) {
sprintf("For tweet id: %s ", tweet_id)
replies = check_replies(tweets_df[tweets_df$ID == tweet_id,])
# Delete blank replies as this causes issues later on with DF operations.
replies <- replies[!grepl("^\\s*$",replies$Text),]
if (!is.null(replies)) {
replies$Topic_category <- 0
# Do sentiment analysis for each reply
replies <- cbind(replies, bind_rows(lapply(replies$Text, sentiment_score)))
sprintf("Sentiment analysis done for all replies on Tweet %s\n", tweet_id)
# Do Topic modeling for all replies.
temp_model <- topic_model(replies$Text)
# Add top terms in all replies corpus to Tweet data
tweets_df[tweets_df$ID == tweet_id,]$Topic_terms_1 <- toString(temp_model$terms[,1])
tweets_df[tweets_df$ID == tweet_id,]$Topic_terms_2 <- toString(temp_model$terms[,2])
# Add Topic category to the reply, determined by highest beta value. Equal values are not classified.
temp_topics <- temp_model$topics %>% group_by(document) %>%
filter(gamma==max(gamma)) %>%
arrange(document, topic)
temp_topics <- temp_topics %>% filter(gamma != 0.5000000) %>% select(document, topic)
for (j in 1:nrow(replies)) {
replies[j,]$Topic_category <- toString(temp_topics[temp_topics$document==toString(j),]$topic)
}
# remove temp_model from memory to save space
rm(list=c('temp_model', 'temp_topics'))
sprintf("Topic Modelling done for Tweet %s", tweet_id)
return(replies)
}
}
# Tweet: 1335971721262796801
reply_model_1335971721262796801 <- get_all_modeling_data('1335971721262796801')
# Tweet: 1335984976521801733
reply_model_1335984976521801733 <- get_all_modeling_data('1335984976521801733')
# Tweet: 1336018515028037635
reply_model_1336018515028037635 <- get_all_modeling_data('1336018515028037635')
# Tweet: 1336069391126044673
reply_model_1336069391126044673 <- get_all_modeling_data('1336069391126044673')
# Tweet: 1335984976521801733
reply_model_1335984976521801733 <- get_all_modeling_data('1335984976521801733')
# Tweet: 1335984976521801733
reply_model_1335984976521801733 <- get_all_modeling_data('1335984976521801733')
# Tweet: 1335984976521801733
reply_model_1335984976521801733 <- get_all_modeling_data('1335984976521801733')
# Tweet: 1335984976521801733
reply_model_1335984976521801733 <- get_all_modeling_data('1335984976521801733')
```{r}
# Tweet: 1335971721262796801
reply_model_1335984976521801733 <- get_all_modeling_data('1335984976521801733')
z <- get_all_modeling_data('1335984976521801733')
z <- get_all_modeling_data("1335984976521801733")
View(tweets_df)
knitr::opts_chunk$set(echo = TRUE)
# C:\Users\Sandeep\Desktop\540\Final project\replies
getwd()
rm(list = ls(all.names = TRUE))
library(files)
library(googleLanguageR)
library(tidytext)
library(stringr)
library(tidyr)
library(tm)
library(syuzhet)
library(topicmodels)
library(tidyverse)
library(slam)
library(textstem)
library(reshape2)
library(reticulate)
replies_index = list.files('replies')
# load tweet and its replies.
tweets_df <-  read.csv('Practice Tweets.csv')
tweets_df$Topic_terms_2 = tweets_df$Topic_terms_1 = ''  # instantiate new column for storing Topic models
# Clean tweets
clean_replies <- function(reply) {
clean_tweet <- str_remove_all(reply, "@.*?(\\s+?|$)|^RT |http.*?(\\s+?|$)") # To remove @twitter handles and Retweet tag.
clean_tweet <- tolower(clean_tweet)
clean_tweet <- str_squish(str_trim(clean_tweet))
# Not removing punctuations as some libraries use emoticons to analyze tweets
clean_tweet <- removeWords(clean_tweet, stopwords('en')) # delete stopwords from text
clean_tweet <- stem_strings(clean_tweet) # stemming. Might need to remove it as it can confuse certain sentiment analyses
return(clean_tweet)
}
# Check if we have replies data for the tweet using conversation id
check_replies <- function(tweet) {
reply_file = paste0(tweet[,3], ".csv")
if (reply_file %in% replies_index){
# reply found, work with tweet
cat("Tweet: ", tweet[,8], " ")
cat("Likes: ", tweet[,7], "and Retweets: ", tweet[,6], ". ")
reply_file <- paste0('replies/', reply_file)
cat("Found replies file: ", reply_file)
reply_df <- read.csv(reply_file)
cat("Number of replies found: ", nrow(reply_df), ". \n")
reply_df$Text <- lapply(reply_df$Text, clean_replies)
return(reply_df)
}
else {
return(NULL)
}
}
sentiment_score <- function(tweet) {
# Syuzhet. Other methods are “bing”, “afinn”, “nrc”, and “stanford”
# browser()
return(get_nrc_sentiment(tweet))
}
# Topic modeling on tweets to 2 topics
topic_model <- function(tweet_col) {
reply_corpus <- Corpus(VectorSource(tweet_col))
# Stemming, stop words have been taken care of earlier
import_mat <- DocumentTermMatrix(reply_corpus, control = list(stemming =TRUE,
stopwords = TRUE,
minWordLength = 3, # This keeps the data out of sync
removeNumbers = TRUE,
removePunctuation =TRUE))
import_weight = tapply(import_mat$v/row_sums(import_mat)[import_mat$i],
import_mat$j,
mean) *
log2(nDocs(import_mat)/col_sums(import_mat > 0))
#ignore very frequent and 0 terms
import_mat = import_mat[ row_sums(import_mat) > 0, ]
# 2 topics and random seed of 7
LDA_gibbs = LDA(import_mat, k = 2, method = "Gibbs",
control = list(seed = 7, burnin = 1000,
thin = 100, iter = 1000))
# See Topic modelings values to verify all indicators are within control and expected range
cat("LDA Gibb's Alpha: ", LDA_gibbs@alpha,
"entropy: ", mean(apply(posterior(LDA_gibbs)$topics, 1, function(z) - sum(z * log(z)))))
# Return lists from models so that they can be added to df. Complete model doesn't work
return(list("topics" = tidy(LDA_gibbs, matrix = "gamma"), "terms" = terms(LDA_gibbs, 20)))
}
get_all_modeling_data <- function(tweet_id) {
sprintf("For tweet id: %s ", tweet_id)
replies = check_replies(tweets_df[tweets_df$ID == tweet_id,])
# Delete blank replies as this causes issues later on with DF operations.
replies <- replies[!grepl("^\\s*$",replies$Text),]
if (!is.null(replies)) {
replies$Topic_category <- 0
# Do sentiment analysis for each reply
replies <- cbind(replies, bind_rows(lapply(replies$Text, sentiment_score)))
sprintf("Sentiment analysis done for all replies on Tweet %s\n", tweet_id)
# Do Topic modeling for all replies.
temp_model <- topic_model(replies$Text)
# Add top terms in all replies corpus to Tweet data
tweets_df[tweets_df$ID == tweet_id,]$Topic_terms_1 <- toString(temp_model$terms[,1])
tweets_df[tweets_df$ID == tweet_id,]$Topic_terms_2 <- toString(temp_model$terms[,2])
# Add Topic category to the reply, determined by highest beta value. Equal values are not classified.
temp_topics <- temp_model$topics %>% group_by(document) %>%
filter(gamma==max(gamma)) %>%
arrange(document, topic)
temp_topics <- temp_topics %>% filter(gamma != 0.5000000) %>% select(document, topic)
for (j in 1:nrow(replies)) {
replies[j,]$Topic_category <- toString(temp_topics[temp_topics$document==toString(j),]$topic)
}
# remove temp_model from memory to save space
rm(list=c('temp_model', 'temp_topics'))
sprintf("Topic Modelling done for Tweet %s", tweet_id)
return(replies)
}
}
# Tweet: 1335971721262796801
reply_model_1335984976521801733 <- get_all_modeling_data('1335984976521801733')
# Tweet: 1335971721262796801
reply_model_1335984976521801733 <- get_all_modeling_data('1335984976521801733')
# Tweet: 1335971721262796801
reply_model_1335984976521801733 <- get_all_modeling_data('1335984976521801733')
reply_model_1335775535717289984 <- get_all_modeling_data('1335775535717289984')
View(tweets_df)
View(tweets_df)
get_all_modeling_data <- function(tweet_id) {
sprintf("For tweet id: %s ", tweet_id)
replies = check_replies(tweets_df[tweets_df$ID == tweet_id,])
# Delete blank replies as this causes issues later on with DF operations.
replies <- replies[!grepl("^\\s*$",replies$Text),]
if (!is.null(replies)) {
replies$Topic_category <- 0
# Do sentiment analysis for each reply
replies <- cbind(replies, bind_rows(lapply(replies$Text, sentiment_score)))
sprintf("Sentiment analysis done for all replies on Tweet %s\n", tweet_id)
# Do Topic modeling for all replies.
temp_model <- topic_model(replies$Text)
# Add top terms in all replies corpus to Tweet data
tweets_df[tweets_df$ID == tweet_id,]$Topic_terms_1 <- toString(temp_model$terms[,1])
tweets_df[tweets_df$ID == tweet_id,]$Topic_terms_2 <- toString(temp_model$terms[,2])
# Add Topic category to the reply, determined by highest beta value. Equal values are not classified.
temp_topics <- temp_model$topics %>% group_by(document) %>%
filter(gamma==max(gamma)) %>%
arrange(document, topic)
temp_topics <- temp_topics %>% filter(gamma != 0.5000000) %>% select(document, topic)
for (j in 1:nrow(replies)) {
replies[j,]$Topic_category <- toString(temp_topics[temp_topics$document==toString(j),]$topic)
}
# remove temp_model from memory to save space
rm(list=c('temp_model', 'temp_topics'))
sprintf("Topic Modelling done for Tweet %s", tweet_id)
return(list("replies" = replies, "tweets_df" = tweets_df))
return(replies)
}
}
# Tweet: 1335775535717289984
temp_reply <- get_all_modeling_data('1335775535717289984')
reply_model_1 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1335971721262796801
temp_reply <- get_all_modeling_data('1335971721262796801')
reply_model_2 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1335984976521801733
temp_reply <- get_all_modeling_data('1335984976521801733')
reply_model_3 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1336018515028037635
temp_reply <- get_all_modeling_data('1336018515028037635')
reply_model_4 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1336069391126044673
temp_reply <- get_all_modeling_data('1336069391126044673')
reply_model_5 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1336110929856040960
temp_reply <- get_all_modeling_data('1336110929856040960')
reply_model_6 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1336112582424473600
temp_reply <- get_all_modeling_data('1336112582424473600')
reply_model_7 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1336113603116752897
temp_reply <- get_all_modeling_data('1336113603116752897')
reply_model_8 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
replies_index
temp_reply <- get_all_modeling_data('1335971721262796801')
get_all_modeling_data('12')
get_all_modeling_data(12)
# Tweet: 1336114633485266944
temp_reply <- get_all_modeling_data('1336114633485266944')
reply_model_9 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
library(files)
library(googleLanguageR)
library(tidytext)
library(stringr)
library(tidyr)
library(tm)
library(syuzhet)
library(topicmodels)
library(tidyverse)
library(slam)
library(textstem)
library(reshape2)
library(reticulate)
replies_index = list.files('replies')
# load tweet and its replies.
tweets_df <-  read.csv('Practice Tweets.csv')
tweets_df$Topic_terms_2 = tweets_df$Topic_terms_1 = ''  # instantiate new column for storing Topic models
# Clean tweets
clean_replies <- function(reply) {
clean_tweet <- str_remove_all(reply, "@.*?(\\s+?|$)|^RT |http.*?(\\s+?|$)") # To remove @twitter handles and Retweet tag.
clean_tweet <- tolower(clean_tweet)
clean_tweet <- str_squish(str_trim(clean_tweet))
# Not removing punctuations as some libraries use emoticons to analyze tweets
clean_tweet <- removeWords(clean_tweet, stopwords('en')) # delete stopwords from text
clean_tweet <- stem_strings(clean_tweet) # stemming. Might need to remove it as it can confuse certain sentiment analyses
return(clean_tweet)
}
# Check if we have replies data for the tweet using conversation id
check_replies <- function(tweet) {
reply_file = paste0(tweet[,3], ".csv")
if (reply_file %in% replies_index){
# reply found, work with tweet
cat("Tweet: ", tweet[,8], " ")
cat("Likes: ", tweet[,7], "and Retweets: ", tweet[,6], ". ")
reply_file <- paste0('replies/', reply_file)
cat("Found replies file: ", reply_file)
reply_df <- read.csv(reply_file)
cat("Number of replies found: ", nrow(reply_df), ". \n")
reply_df$Text <- lapply(reply_df$Text, clean_replies)
return(reply_df)
}
else {
return(NULL)
}
}
sentiment_score <- function(tweet) {
# Syuzhet. Other methods are “bing”, “afinn”, “nrc”, and “stanford”
# browser()
return(get_nrc_sentiment(tweet))
}
# Topic modeling on tweets to 2 topics
topic_model <- function(tweet_col) {
reply_corpus <- Corpus(VectorSource(tweet_col))
# Stemming, stop words have been taken care of earlier
import_mat <- DocumentTermMatrix(reply_corpus, control = list(stemming =TRUE,
stopwords = TRUE,
minWordLength = 3, # This keeps the data out of sync
removeNumbers = TRUE,
removePunctuation =TRUE))
import_weight = tapply(import_mat$v/row_sums(import_mat)[import_mat$i],
import_mat$j,
mean) *
log2(nDocs(import_mat)/col_sums(import_mat > 0))
#ignore very frequent and 0 terms
import_mat = import_mat[ row_sums(import_mat) > 0, ]
# 2 topics and random seed of 7
LDA_gibbs = LDA(import_mat, k = 2, method = "Gibbs",
control = list(seed = 7, burnin = 1000,
thin = 100, iter = 1000))
# See Topic modelings values to verify all indicators are within control and expected range
cat("LDA Gibb's Alpha: ", LDA_gibbs@alpha,
"entropy: ", mean(apply(posterior(LDA_gibbs)$topics, 1, function(z) - sum(z * log(z)))))
# Return lists from models so that they can be added to df. Complete model doesn't work
return(list("topics" = tidy(LDA_gibbs, matrix = "gamma"), "terms" = terms(LDA_gibbs, 20)))
}
get_all_modeling_data <- function(tweet_id) {
tweet_id='1335971721262796801'
sprintf("For tweet id: %s ", tweet_id)
replies = check_replies(tweets_df[tweets_df$ID == tweet_id,])
# Delete blank replies as this causes issues later on with DF operations.
replies <- replies[!grepl("^\\s*$",replies$Text),]
if (!is.null(replies)) {
replies$Topic_category <- 0
# Do sentiment analysis for each reply
replies <- cbind(replies, bind_rows(lapply(replies$Text, sentiment_score)))
sprintf("Sentiment analysis done for all replies on Tweet %s\n", tweet_id)
# Do Topic modeling for all replies.
temp_model <- topic_model(replies$Text)
# Add top terms in all replies corpus to Tweet data
tweets_df[tweets_df$ID == tweet_id,]$Topic_terms_1 <- toString(temp_model$terms[,1])
tweets_df[tweets_df$ID == tweet_id,]$Topic_terms_2 <- toString(temp_model$terms[,2])
# Add Topic category to the reply, determined by highest beta value. Equal values are not classified.
temp_topics <- temp_model$topics %>% group_by(document) %>%
filter(gamma==max(gamma)) %>%
arrange(document, topic)
temp_topics <- temp_topics %>% filter(gamma != 0.5000000) %>% select(document, topic)
for (j in 1:nrow(replies)) {
replies[j,]$Topic_category <- toString(temp_topics[temp_topics$document==toString(j),]$topic)
}
# remove temp_model from memory to save space
rm(list=c('temp_model', 'temp_topics'))
sprintf("Topic Modelling done for Tweet %s", tweet_id)
return(list("replies" = replies, "tweets_df" = tweets_df))
}
}
# Tweet: 1336114633485266944
temp_reply <- get_all_modeling_data('1336114633485266944')
reply_model_9 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1336114633485266944
temp_reply <- get_all_modeling_data('1336114633485266944')
reply_model_9 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1336114633485266944
temp_reply <- get_all_modeling_data('1336110929856040960')
reply_model_9 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1336114633485266944
temp_reply <- get_all_modeling_data('1335775535717289984')
reply_model_9 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
# Tweet: 1336114633485266944
temp_reply <- get_all_modeling_data('1335775535717289984')
reply_model_9 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
temp_reply <- get_all_modeling_data('1335775535717289984')
reply_model_1 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
temp_reply <- get_all_modeling_data('1335775535717289984')
reply_model_1 <- temp_reply$replies
tweets_df <- temp_reply$tweets_df
